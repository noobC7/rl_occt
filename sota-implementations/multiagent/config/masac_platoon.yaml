seed: 40

env:
  max_steps: None
  eval_max_steps: 800
  scenario_name: "occt_scenario"
  scenario:
    viewer_zoom: 17.0
    resolution_factor: 2
    eval_mode: ??? # automatically fix the map path batch id
    n_agents: 4
    task_class: 0
    is_rand_arc_pos: False
    init_vel_mean: 3.0
    init_vel_std: 1.0 
    still_space: 10
    init_arc_pos: 0.0 
    use_center_frenet_ref: True 
    n_nearing_agents_observed: 2
    # reward weights (Consistent with MAPPO baseline)
    reward_progress: 10.0
    reward_vel: 0
    reward_goal: 100
    reward_track_ref_vel: 20       
    reward_track_ref_space: 50
    reward_track_ref_heading: 0
    reward_track_ref_path: 50
    penalty_near_boundary: -100
    penalty_near_other_agents: -100
    penalty_change_steering: -0.0
    penalty_change_acc: -0.0
    threshold_change_steering: 0.0
    threshold_change_acc: 0.0
    penalty_collide_with_agents: -100
    penalty_outside_boundaries: -100
    penalty_backward: -100
  device: ??? 
  vmas_envs: 300 # SAC often needs more updates per sample, so fewer parallel envs can save memory

model:
  shared_parameters: True
  centralised_critic: True  # MASAC if True, ISAC if False

collector:
  frames_per_batch: 60_000 # Frames sampled each sampling iteration
  n_iters: 500 # Number of sampling/training iterations
  total_frames: ???

buffer:
  memory_size: 600000 # Larger buffer for SAC

loss:
  gamma: 0.99
  lmbda: 0.9
  tau: 0.005 # Soft update coefficient
  # SAC doesn't use lmbda (GAE), it's off-policy

train:
  num_epochs: 2  # optimization steps per batch of data collected
  minibatch_size: 4096 # size of minibatches used in each epoch
  lr: 2e-5
  max_grad_norm: 1.0
  device: ???
  resume_from_checkpoint: None

eval:
  evaluation_interval: 10
  evaluation_episodes: 13

logger:
  backend: swanlab 
  project_name: "MASAC_PLATOON"
  group_name: "masac_baseline"
  resume_swanlab_id: None # copy from swanlab page
  mode: local